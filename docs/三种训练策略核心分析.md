# 三种训练策略核心内容分析

本文档详细分析了 HIL-SERL 框架中的三种控制策略训练脚本：`train_bc.py`、`train_hgdagger.py` 和 `train_rlpd.py`。

---

## 一、train_bc.py - 行为克隆（Behavior Cloning）

### 1.1 核心思想

**行为克隆（BC）**是一种纯粹的监督学习方法，通过模仿专家演示数据来学习策略。它不涉及环境交互，完全依赖预收集的演示数据。

### 1.2 关键特性

- **算法类型**：监督学习（Supervised Learning）
- **智能体类型**：`BCAgent`（行为克隆智能体）
- **训练模式**：离线训练（Offline Training）
- **数据来源**：预收集的演示数据（`demo_data/*.pkl`）
- **架构**：单进程训练（无 Actor-Learner 分离）

### 1.3 核心代码流程

#### 1.3.1 数据加载（第175-185行）
```python
demo_path = glob.glob(os.path.join(os.getcwd(), "demo_data", "*.pkl"))
for path in demo_path:
    with open(path, "rb") as f:
        transitions = pkl.load(f)
        for transition in transitions:
            if np.linalg.norm(transition['actions']) > 0.0:
                bc_replay_buffer.insert(transition)
```
- 从 `demo_data/` 目录加载所有演示数据
- 过滤掉零动作的转换（`np.linalg.norm(transition['actions']) > 0.0`）
- 将数据插入到回放缓冲区

#### 1.3.2 训练循环（第110-123行）
```python
for step in tqdm.tqdm(range(FLAGS.train_steps), ...):
    batch = next(bc_replay_iterator)
    bc_agent, bc_update_info = bc_agent.update(batch)
    if step % config.log_period == 0 and wandb_logger:
        wandb_logger.log({"bc": bc_update_info}, step=step)
    if step > FLAGS.train_steps - 100 and step % 10 == 0:
        checkpoints.save_checkpoint(...)
```
- 从演示数据中采样批次
- 使用监督学习更新策略（最小化动作预测误差）
- 定期保存检查点

#### 1.3.3 评估模式（第57-88行）
```python
def eval(env, bc_agent, sampling_rng):
    for episode in range(FLAGS.eval_n_trajs):
        obs, _ = env.reset()
        while not done:
            actions = bc_agent.sample_actions(observations=obs, seed=key)
            next_obs, reward, done, truncated, info = env.step(actions)
```
- 加载训练好的检查点
- 在环境中运行策略
- 统计成功率和平均时间

### 1.4 优缺点

**优点**：
- 实现简单，训练快速
- 不需要环境交互，安全可靠
- 适合作为预训练策略

**缺点**：
- 无法处理分布外（OOD）情况
- 没有探索能力
- 容易产生复合误差（compounding error）
- 完全依赖演示数据质量

---

## 二、train_hgdagger.py - 人类引导的DAgger（Human-Guided DAgger）

### 2.1 核心思想

**HG-DAgger** 是 DAgger（Dataset Aggregation）算法的变体，结合了在线学习和人类干预。智能体在环境中执行动作，当出现错误时，人类专家可以实时干预，纠正动作。只有被干预的轨迹数据才会被用于训练。

### 2.2 关键特性

- **算法类型**：在线监督学习（Online Supervised Learning）
- **智能体类型**：`BCAgent`（行为克隆智能体）
- **训练模式**：在线训练（Online Training）+ 人类干预
- **数据来源**：初始演示数据 + 在线干预数据
- **架构**：Actor-Learner 分离架构（分布式训练）

### 2.3 核心代码流程

#### 2.3.1 Actor 进程 - 数据收集（第78-232行）

**关键机制：人类干预检测**
```python
# override the action with the intervention action
if "intervene_action" in info:
    actions = info.pop("intervene_action")
    intervention_steps += 1
    if not already_intervened:
        intervention_count += 1
    already_intervened = True
else:
    already_intervened = False

# 只有被干预的转换才被存储
if already_intervened:
    data_store.insert(transition)
    demo_transitions.append(copy.deepcopy(transition))
```

**核心逻辑**：
1. 智能体根据当前策略采样动作
2. 环境执行动作，返回 `info` 字典
3. 如果 `info` 中包含 `"intervene_action"`，说明人类进行了干预
4. **只存储被干预的转换数据**（第205-207行）
5. 所有数据（包括干预数据）都发送到 Learner

#### 2.3.2 Learner 进程 - 模型训练（第242-332行）

**预训练阶段**（第271-304行）：
```python
if FLAGS.pretrain_steps:
    # 从初始演示数据预训练
    for step in tqdm.tqdm(range(FLAGS.pretrain_steps), ...):
        batch = next(demo_iterator)
        agent, bc_update_info = agent.update(batch)
```

**在线训练阶段**（第311-331行）：
```python
for step in tqdm.tqdm(range(FLAGS.pretrain_steps+1, config.max_steps), ...):
    batch = next(demo_iterator)  # 只从干预数据中采样
    agent, update_info = agent.update(batch)
    
    # 定期发布更新后的网络参数
    if step % config.steps_per_update == 0:
        server.publish_network(agent.state.params)
```

**关键点**：
- **只使用干预数据进行训练**（`demo_iterator` 来自 `demo_buffer`，只包含干预数据）
- 定期将更新后的参数发送给 Actor
- 支持检查点恢复和继续训练

#### 2.3.3 数据持久化（第223-230行）
```python
if step > 0 and config.buffer_period > 0 and step % config.buffer_period == 0:
    demo_buffer_path = os.path.join(FLAGS.checkpoint_path, "demo_buffer")
    with open(os.path.join(demo_buffer_path, f"transitions_{step}.pkl"), "wb") as f:
        pkl.dump(demo_transitions, f)
```
- 定期将干预数据保存到磁盘
- 支持训练中断后恢复

### 2.4 与 BC 的对比

| 特性 | train_bc.py | train_hgdagger.py |
|------|-------------|-------------------|
| 训练方式 | 离线（仅演示数据） | 在线（演示+干预数据） |
| 数据来源 | 预收集演示 | 初始演示 + 实时干预 |
| 架构 | 单进程 | Actor-Learner 分离 |
| 探索能力 | 无 | 有（但依赖人类纠正） |
| 人类参与 | 仅数据收集阶段 | 整个训练过程 |

### 2.5 优缺点

**优点**：
- 可以处理分布外情况（通过人类干预）
- 持续改进策略性能
- 人类专家可以实时纠正错误
- 只学习高质量数据（干预数据）

**缺点**：
- 需要人类专家持续参与
- 训练效率取决于干预频率
- 可能产生大量无效探索

---

## 三、train_rlpd.py - 强化学习与预训练演示（RLPD）

### 3.1 核心思想

**RLPD（Reinforcement Learning from Pre-training and Demonstrations）** 结合了强化学习和演示学习。它使用 SAC（Soft Actor-Critic）算法进行在线强化学习，同时混合使用演示数据来引导学习过程。

### 3.2 关键特性

- **算法类型**：强化学习（Reinforcement Learning）+ 演示学习
- **智能体类型**：`SACAgent` / `SACAgentHybridSingleArm` / `SACAgentHybridDualArm`
- **训练模式**：在线强化学习 + 演示数据混合训练
- **数据来源**：初始演示数据 + 在线经验 + 干预数据
- **架构**：Actor-Learner 分离架构（分布式训练）
- **采样策略**：50/50 混合（50% 在线经验 + 50% 演示数据）

### 3.3 核心代码流程

#### 3.3.1 Actor 进程 - 数据收集（第68-241行）

**探索策略**（第158-167行）：
```python
if step < config.random_steps:
    actions = env.action_space.sample()  # 随机探索
else:
    actions = agent.sample_actions(
        observations=jax.device_put(obs),
        seed=key,
        argmax=False,
    )
```
- 初始阶段使用随机探索
- 之后使用策略网络采样动作

**数据存储机制**（第197-203行）：
```python
# 所有转换都存储到主回放缓冲区
data_store.insert(transition)
transitions.append(copy.deepcopy(transition))

# 干预数据额外存储到演示缓冲区
if already_intervened:
    intvn_data_store.insert(transition)
    demo_transitions.append(copy.deepcopy(transition))
```

**关键点**：
- **所有数据**（包括随机探索和策略执行）都存储到 `data_store`
- **干预数据**额外存储到 `intvn_data_store`
- 支持两种数据流的独立管理

#### 3.3.2 Learner 进程 - 模型训练（第246-356行）

**数据采样策略**（第289-303行）：
```python
# 50/50 混合采样
replay_iterator = replay_buffer.get_iterator(
    sample_args={
        "batch_size": config.batch_size // 2,  # 一半来自在线经验
        "pack_obs_and_next_obs": True,
    },
    device=sharding.replicate(),
)
demo_iterator = demo_buffer.get_iterator(
    sample_args={
        "batch_size": config.batch_size // 2,  # 一半来自演示数据
        "pack_obs_and_next_obs": True,
    },
    device=sharding.replicate(),
)
```

**训练循环**（第315-339行）：
```python
for step in tqdm.tqdm(range(start_step, config.max_steps), ...):
    # 运行 n-1 次仅更新 Critic
    for critic_step in range(config.cta_ratio - 1):
        batch = next(replay_iterator)
        demo_batch = next(demo_iterator)
        batch = concat_batches(batch, demo_batch, axis=0)  # 合并批次
        
        agent, critics_info = agent.update(
            batch,
            networks_to_update=train_critic_networks_to_update,  # 只更新 Critic
        )
    
    # 最后一次同时更新 Critic 和 Actor
    batch = next(replay_iterator)
    demo_batch = next(demo_iterator)
    batch = concat_batches(batch, demo_batch, axis=0)
    
    agent, update_info = agent.update(
        batch,
        networks_to_update=train_networks_to_update,  # 更新 Critic + Actor
    )
```

**关键机制**：
1. **50/50 混合采样**：每个批次由 50% 在线经验和 50% 演示数据组成
2. **Critic-Actor 更新比例**：使用 `cta_ratio` 控制 Critic 和 Actor 的更新频率
3. **网络更新控制**：可以指定更新哪些网络（Critic、Actor、Temperature）

#### 3.3.3 多种智能体支持（第386-417行）

```python
if config.setup_mode == 'single-arm-fixed-gripper' or config.setup_mode == 'dual-arm-fixed-gripper':
    agent = make_sac_pixel_agent(...)  # 标准 SAC
elif config.setup_mode == 'single-arm-learned-gripper':
    agent = make_sac_pixel_agent_hybrid_single_arm(...)  # 混合 SAC（单臂）
elif config.setup_mode == 'dual-arm-learned-gripper':
    agent = make_sac_pixel_agent_hybrid_dual_arm(...)  # 混合 SAC（双臂）
```

支持不同的任务设置：
- **固定夹爪**：标准 SAC 算法
- **学习夹爪**：混合 SAC，包含额外的 `grasp_critic` 网络

### 3.4 与 BC 和 HG-DAgger 的对比

| 特性 | train_bc.py | train_hgdagger.py | train_rlpd.py |
|------|-------------|-------------------|---------------|
| **算法** | 监督学习 | 在线监督学习 | 强化学习 + 演示 |
| **智能体** | BCAgent | BCAgent | SACAgent |
| **数据使用** | 仅演示 | 仅干预数据 | 在线经验 + 演示（50/50） |
| **探索** | 无 | 有（人类纠正） | 有（自主探索） |
| **奖励信号** | 无 | 无 | 有（环境奖励） |
| **人类参与** | 数据收集 | 持续干预 | 可选干预 |
| **训练复杂度** | 低 | 中 | 高 |

### 3.5 优缺点

**优点**：
- 结合了强化学习和演示学习的优势
- 可以自主探索和学习
- 演示数据提供高质量引导
- 支持人类干预进行实时纠正
- 适合复杂任务和长期学习

**缺点**：
- 训练复杂度高
- 需要调优超参数（混合比例、更新频率等）
- 计算资源需求大
- 可能产生不安全的探索行为

---

## 四、三种策略的选择建议

### 4.1 使用场景

**train_bc.py（行为克隆）**：
- ✅ 有大量高质量演示数据
- ✅ 任务相对简单，状态空间有限
- ✅ 需要快速获得基础策略
- ✅ 作为其他方法的预训练

**train_hgdagger.py（HG-DAgger）**：
- ✅ 需要人类专家持续参与
- ✅ 任务复杂，但可以实时纠正
- ✅ 希望只学习高质量数据
- ✅ 需要快速迭代改进策略

**train_rlpd.py（RLPD）**：
- ✅ 需要长期自主学习和改进
- ✅ 任务复杂，需要探索
- ✅ 有环境奖励信号
- ✅ 希望结合演示和强化学习优势

### 4.2 典型训练流程

1. **阶段1：BC 预训练**
   - 使用 `train_bc.py` 从演示数据预训练基础策略
   - 获得初始策略检查点

2. **阶段2：HG-DAgger 或 RLPD 微调**
   - **选项A**：使用 `train_hgdagger.py` 进行人类引导的在线学习
   - **选项B**：使用 `train_rlpd.py` 进行强化学习微调

3. **阶段3：持续改进**
   - 根据任务特点选择 HG-DAgger 或 RLPD
   - 持续收集数据并改进策略

---

## 五、关键技术细节

### 5.1 Actor-Learner 通信机制

**HG-DAgger 和 RLPD 都使用 Actor-Learner 分离架构**：

```python
# Actor 端
client = TrainerClient(
    "actor_env",
    FLAGS.ip,
    make_trainer_config(),
    data_store,
    wait_for_server=True,
)

# Learner 端
server = TrainerServer(make_trainer_config(), request_callback=stats_callback)
server.register_data_store("actor_env", replay_buffer)
server.start(threaded=True)
server.publish_network(agent.state.params)  # 发布网络参数
```

- **Actor**：收集数据，接收 Learner 更新的参数
- **Learner**：训练模型，发布更新后的参数
- **通信**：使用 `agentlace` 框架的 `TrainerClient`/`TrainerServer`

### 5.2 人类干预机制

所有三种方法都支持人类干预，但使用方式不同：

- **BC**：干预发生在数据收集阶段（`record_demos.py`）
- **HG-DAgger**：干预发生在训练过程，只存储干预数据
- **RLPD**：干预发生在训练过程，干预数据单独存储，与在线经验混合使用

### 5.3 数据管理

- **BC**：单缓冲区（演示数据）
- **HG-DAgger**：单缓冲区（干预数据）
- **RLPD**：双缓冲区（在线经验 + 演示数据）

---

## 六、总结

三种训练策略各有特点：

1. **BC**：简单快速，适合预训练和简单任务
2. **HG-DAgger**：人类引导，适合需要持续改进的复杂任务
3. **RLPD**：强化学习 + 演示，适合需要长期自主学习的复杂任务

选择哪种策略取决于：
- 任务复杂度
- 数据可用性
- 人类参与程度
- 计算资源
- 性能要求

在实际应用中，通常采用组合策略：先用 BC 预训练，再用 HG-DAgger 或 RLPD 进行微调和持续改进。
