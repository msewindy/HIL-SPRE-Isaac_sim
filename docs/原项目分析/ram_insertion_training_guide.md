# RAM 插入任务训练详细指南

本文档详细描述了使用 HIL-SERL 框架在 Franka 机械臂上训练 RAM 插入任务的完整流程。RAM 插入是一个典型的精密装配任务，需要机器人将内存条精确插入到主板的插槽中。

## 目录

1. [准备工作](#准备工作)
2. [机器人设置](#机器人设置)
3. [训练配置编辑](#训练配置编辑)
4. [奖励分类器训练](#奖励分类器训练)
5. [演示数据收集](#演示数据收集)
6. [策略训练](#策略训练)
7. [策略评估](#策略评估)
8. [训练技巧与注意事项](#训练技巧与注意事项)

---

## 硬件清单

在开始训练之前，请确保已准备好以下硬件设备：

### 核心机器人硬件

1. **Franka 机械臂**
   - 型号：Franka Panda 或 Franka Emika FR3
   - 要求：已安装并校准，能够正常运行 FCI（Franka Control Interface）模式
   - 状态：机器人控制箱已上电，机器人已解锁

2. **夹爪（Gripper）**
   - 选项 1：Robotiq 夹爪（需要独立 IP 地址）
   - 选项 2：Franka 原装夹爪
   - 选项 3：无夹爪（仅用于特定任务）
   - 要求：能够正常开合，用于抓取 RAM 条

3. **末端执行器相机**
   - 数量：至少 2 个 RealSense 相机
   - 安装位置：手腕/末端执行器上
   - 要求：
     - 能够通过 RealSense Viewer 识别和连接
     - 记录每个相机的序列号（Serial Number）
     - 相机重量需要在 Franka Desk 中配置到末端执行器质量参数中

### 人机交互设备

4. **SpaceMouse（3D 鼠标）**
   - 用途：用于人机干预和演示数据收集
   - 重要性：强烈建议使用，键盘等设备精度不够
   - 要求：已安装驱动，能够正常控制机器人

### 任务相关硬件

5. **主板（Motherboard）**
   - 要求：
     - 带有 RAM 插槽
     - **必须固定在特定位置**（位置由 TARGET_POSE 确定，不能随意移动）
     - 位置精度要求高，因为任务空间边界框很小（约 ±3cm）

6. **RAM 条（内存条）**
   - 要求：能够插入到主板的 RAM 插槽中
   - 数量：至少 1 条（用于训练和测试）

7. **RAM 支架/托盘**
   - 用途：用于放置待抓取的 RAM 条
   - 要求：
     - **必须固定在特定位置**（位置由 GRASP_POSE 确定）
     - 能够稳定支撑 RAM 条，便于机械臂抓取

### 计算和网络设备

8. **计算机/工作站**
   - 要求：
     - 运行 Linux 系统（推荐 Ubuntu）
     - 已安装 Python 3.10、Conda 环境
     - 已安装 JAX（支持 CPU、GPU 或 TPU）
     - 已安装 ROS（用于机器人控制）
     - 已安装 `serl_franka_controllers` 包

9. **网络连接**
   - 要求：
     - 机器人、夹爪（如使用 Robotiq）和计算机在同一网络中
     - 能够通过 IP 地址访问机器人
     - 网络延迟低，确保实时控制

### 工作空间要求

10. **工作空间布局**
    - 机械臂工作空间：确保有足够的空间进行 RAM 插入操作
    - 固定安装：主板和 RAM 支架必须**固定安装**在特定位置
    - 安全区域：确保工作空间内没有障碍物，边界框设置合理
    - 照明：确保相机能够清晰拍摄任务场景

### 硬件配置检查清单

在开始训练前，请确认：

- [ ] Franka 机械臂已上电、解锁，FCI 模式可用
- [ ] 夹爪已连接并测试开合功能正常
- [ ] 至少 2 个 RealSense 相机已安装到手腕，能够正常采集图像
- [ ] 相机序列号已记录
- [ ] 末端执行器质量参数已在 Franka Desk 中配置（包含相机重量）
- [ ] SpaceMouse 已连接并测试可用
- [ ] 主板已固定在目标位置（TARGET_POSE 对应位置）
- [ ] RAM 支架已固定在抓取位置（GRASP_POSE 对应位置）
- [ ] 计算机已安装所有必要的软件和依赖
- [ ] 网络连接正常，能够访问机器人 IP
- [ ] 工作空间安全，无障碍物

---

## 准备工作

在开始训练之前，请确保已完成以下准备工作：

1. **环境安装**：按照主 README.md 中的说明完成 Python 环境设置和 Franka 控制器安装
2. **激活环境**：确保已激活 `hilserl` conda 环境
3. **硬件检查**：按照上述硬件清单检查所有设备是否就绪
4. **工作空间准备**：参考论文中的工作空间设置图片，准备好 RAM 插入任务所需的工作空间，确保主板和 RAM 支架固定在正确位置

---

## 机器人设置

### 步骤 1：调整末端执行器质量

由于手腕上安装了相机，需要调整末端执行器的质量参数：

1. 打开 Franka Desk 界面
2. 进入 `Desk > Settings > End-effector > Mechanical Data > Mass`
3. 根据实际安装的相机重量调整质量参数

这一步对于阻抗控制器的准确性至关重要。

### 步骤 2：启动机器人服务器

1. **解锁机器人并激活 FCI**：
   - 在 Franka Desk 中解锁机器人
   - 激活 FCI（Franka Control Interface）模式

2. **编辑启动脚本**：
   - 找到启动文件：`serl_robot_infra/robot_servers/launch_right_server.sh`
   - 编辑以下内容：
     - `setup.bash` 路径：指向包含 `serl_franka_controllers` 包的 catkin_ws 的 devel/setup.bash
     - `python franka_server.py` 命令的参数：
       - `--gripper_type`：设置夹爪类型（Robotiq/Franka/None）
       - `--robot_ip`：机器人 IP 地址
       - `--gripper_ip`：夹爪 IP 地址（如果使用 Robotiq 夹爪）
       - `--flask_url`：Flask 服务器 URL
       - `--ros_port`：ROS 端口号
   
   详细参数说明请参考 `serl_robot_infra/README.md`

3. **启动服务器**：
```bash
bash serl_robot_infra/robot_servers/launch_right_server.sh
```

启动成功后，你应该能够通过移动末端执行器来测试阻抗控制器是否正常工作（如果控制器运行正常，末端执行器应该是柔顺的）。

---

## 训练配置编辑

训练配置位于 `examples/experiments/ram_insertion/config.py`。该文件包含两个主要配置类：`EnvConfig` 和 `TrainConfig`。

### 步骤 3：配置服务器 URL

在 `EnvConfig` 类中，修改 `SERVER_URL` 为运行中的 Franka 服务器 URL：

```python
class EnvConfig(DefaultEnvConfig):
    SERVER_URL = "http://127.0.0.2:5000/"  # 修改为你的服务器 URL
```

### 步骤 4：配置相机

RAM 插入任务使用两个手腕相机。配置步骤如下：

1. **获取相机序列号**：
   - 使用 RealSense Viewer 应用程序查看已连接的相机
   - 记录每个相机的序列号

2. **更新相机配置**：
   在 `EnvConfig` 类中更新 `REALSENSE_CAMERAS`：
```python
REALSENSE_CAMERAS = {
    "wrist_1": {
        "serial_number": "你的相机1序列号",
        "dim": (1280, 720),
        "exposure": 40000,
    },
    "wrist_2": {
        "serial_number": "你的相机2序列号",
        "dim": (1280, 720),
        "exposure": 40000,
    },
}
```

3. **配置图像裁剪**：
   在 `IMAGE_CROP` 中设置每个相机的图像裁剪区域：
```python
IMAGE_CROP = {
    "wrist_1": lambda img: img[150:450, 350:1100],  # 根据实际需要调整
    "wrist_2": lambda img: img[100:500, 400:900],   # 根据实际需要调整
}
```

4. **配置相机使用**：
   在 `TrainConfig` 类中确认相机键的使用：
```python
image_keys = ["wrist_1", "wrist_2"]        # 用于策略训练的相机
classifier_keys = ["wrist_1", "wrist_2"]   # 用于奖励分类器的相机
```

**提示**：要调整图像裁剪和曝光参数，可以运行奖励分类器数据收集脚本（步骤 6）或演示数据收集脚本（步骤 8）来可视化相机输入。

### 步骤 5：收集关键位姿

需要收集以下关键位姿用于训练过程：

1. **TARGET_POSE**：机械臂抓取已完全插入主板的 RAM 条时的位姿
2. **GRASP_POSE**：机械臂抓取放置在支架上的 RAM 条时的位姿
3. **RESET_POSE**：重置时的机械臂位姿

**收集位姿的方法**：

使用以下命令获取当前机械臂位姿：
```bash
curl -X POST http://<FRANKA_SERVER_URL>:5000/getpos_euler
```

返回的格式为：`[x, y, z, roll, pitch, yaw]`

**配置位姿参数**：

在 `EnvConfig` 类中更新位姿：
```python
TARGET_POSE = np.array([x, y, z, roll, pitch, yaw])  # RAM 完全插入时的位姿
GRASP_POSE = np.array([x, y, z, roll, pitch, yaw])    # 抓取 RAM 时的位姿
RESET_POSE = TARGET_POSE + np.array([0, 0, 0.05, 0, 0.05, 0])  # 重置位姿
```

**配置探索边界**：

设置策略的探索边界框：
```python
ABS_POSE_LIMIT_LOW = TARGET_POSE - np.array([0.03, 0.02, 0.01, 0.01, 0.1, 0.4])
ABS_POSE_LIMIT_HIGH = TARGET_POSE + np.array([0.03, 0.02, 0.05, 0.01, 0.1, 0.4])
```

这些边界定义了策略可以探索的安全区域，确保机器人不会移动到危险位置。

**配置随机重置**：

任务启用了随机重置功能，每次重置时会在 `RESET_POSE` 周围添加随机化：
```python
RANDOM_RESET = True
RANDOM_XY_RANGE = 0.02   # XY 方向的随机范围（米）
RANDOM_RZ_RANGE = 0.05   # 绕 Z 轴旋转的随机范围（弧度）
```

随机重置有助于提高策略的鲁棒性。

---

## 奖励分类器训练

奖励分类器用于自动判断任务是否成功完成。对于 RAM 插入任务，我们使用与策略训练相同的两个手腕相机图像来训练奖励分类器。

### 步骤 6：收集分类器训练数据

1. **进入 examples 目录**：
```bash
cd examples
```

2. **运行数据收集脚本**：
```bash
python record_success_fail.py --exp_name ram_insertion --successes_needed 200
```

**数据收集过程**：

- 脚本运行后，所有记录的转换（transitions）默认标记为**负样本**（无奖励）
- 当 RAM 完全插入时，**按住空格键**，该转换将被标记为**正样本**（有奖励）
- 脚本会在收集到足够的正样本后自动终止（默认 200 个，可通过 `--successes_needed` 参数调整）

**收集策略**：

- **负样本收集**：
  - RAM 条在工作空间中的各种位置（未插入状态）
  - 插入过程中的中间状态
  - 插入到错误位置
  - 只插入一半的情况
  - RAM 条紧挨着插槽但未插入

- **正样本收集**：
  - 仅在 RAM 完全插入主板插槽时按住空格键

**重要提示**：为了训练一个能够抵抗误报的分类器（这对训练成功的策略很重要），建议收集 **2-3 倍于正样本数量的负样本**，以覆盖所有失败模式。

**数据保存位置**：
收集的数据将保存到 `experiments/ram_insertion/classifier_data` 文件夹。

### 步骤 7：训练奖励分类器

1. **进入实验目录**：
```bash
cd experiments/ram_insertion
```

2. **运行分类器训练脚本**：
```bash
python ../../train_reward_classifier.py --exp_name ram_insertion
```

**训练过程**：

- 分类器将在 `TrainConfig` 中 `classifier_keys` 指定的相机图像上训练
- 训练完成后，分类器将保存到 `experiments/ram_insertion/classifier_ckpt` 文件夹

**分类器工作原理**：

训练好的分类器会输出一个分数，通过 sigmoid 函数转换为概率。在策略训练和演示收集时，如果概率超过阈值（默认 0.85）且满足其他条件（如 z 位置检查），则认为任务成功。

---

## 演示数据收集

少量的人类演示对于加速强化学习过程至关重要。对于 RAM 插入任务，我们使用 20 个演示。

### 步骤 8：录制演示数据

1. **确保已训练奖励分类器**：
   演示收集脚本会使用奖励分类器来判断演示是否成功。

2. **运行演示录制脚本**：
```bash
cd examples
python record_demos.py --exp_name ram_insertion --successes_needed 20
```

**录制过程**：

- 使用 SpaceMouse 控制机器人执行任务
- 当奖励分类器判断任务成功或回合超时后，机器人会自动重置
- 脚本会在收集到 20 个成功的演示后自动终止
- 演示数据将保存到 `experiments/ram_insertion/demo_data` 文件夹

**重要提示**：

在演示数据收集过程中，你可能会注意到奖励分类器出现以下情况：

- **误报（False Positives）**：回合在没有成功插入的情况下被判定为成功
- **漏报（False Negatives）**：尽管成功插入，但没有给出奖励

如果出现这些问题，应该：

1. **优先选择**：收集额外的分类器数据，针对观察到的分类器失败模式（例如，如果分类器对在空中持握 RAM 条的情况给出误报，应该收集更多该情况的负样本数据）
2. **备选方案**：调整奖励分类器阈值，但强烈建议先收集更多分类器数据（如果需要，甚至可以添加更多分类器相机/图像）

---

## 策略训练

策略训练采用异步架构，包括两个节点：

- **Actor 节点**：在环境中执行策略，收集转换数据并发送给 Learner
- **Learner 节点**：训练策略，将更新后的策略发送回 Actor

两个节点需要同时运行。

### 步骤 9：配置并启动训练

1. **编辑启动脚本**：

   在 `experiments/ram_insertion/` 目录下找到两个脚本：
   - `run_actor.sh`：Actor 节点启动脚本
   - `run_learner.sh`：Learner 节点启动脚本

2. **配置检查点路径**：

   在两个脚本中编辑 `checkpoint_path`，指向保存检查点和其他训练数据的文件夹：
```bash
--checkpoint_path=/path/to/your/checkpoint/folder
```

3. **配置演示数据路径**：

   在 `run_learner.sh` 中编辑 `demo_path`，指向录制的演示数据：
```bash
--demo_path=/path/to/demo_data/demo_file.pkl
```

   如果有多个演示文件，可以提供多个 `--demo_path` 参数。

4. **启动训练**：

   在**两个不同的终端**中分别运行：
```bash
# 终端 1：启动 Actor
bash run_actor.sh

# 终端 2：启动 Learner
bash run_learner.sh
```

**恢复训练**：

如果需要恢复之前的训练运行，只需将 `checkpoint_path` 指向之前运行的文件夹，代码会自动加载最新的检查点和训练缓冲区数据，继续训练。

### 步骤 10：训练过程中的干预

在训练过程中，你应该根据需要使用 SpaceMouse 进行干预，特别是在训练初期或策略反复探索错误行为时。

**干预策略**：

1. **训练初期**：
   - 频繁进行干预（可以每个回合或每隔几个回合干预一些时间步）
   - 在探索和干预之间找到平衡：让策略探索（这对 RL 至关重要），同时通过干预引导其高效探索
   - 例如，对于插入任务，在训练初期策略会有很多随机运动。通常让策略探索 20-30 个时间步，然后干预引导物体接近插入口，让策略在那里练习插入
   - 交替进行探索和干预，让策略有机会探索任务的每个部分，同时不会浪费时间探索完全错误的行为

2. **提高奖励频率**：
   - 在训练初期，建议通过干预帮助策略完成任务并获得奖励，频率可以较高（例如让 1/3 或更多的回合获得奖励）
   - 频繁的奖励有助于价值备份传播更快，加速训练

3. **训练中后期**：
   - 当策略开始表现更合理（能够偶尔独立成功完成任务，几乎不需要干预）时，可以显著减少干预频率
   - 在这个阶段，主要是在策略反复犯同样错误时才进行干预

4. **鲁棒性训练**：
   - 如果想要训练的策略具有更强的重试行为（即使早期犯错也能成功完成任务）或对外部干扰更鲁棒，可以使用干预来帮助策略练习这些边缘情况
   - 例如，如果希望 USB 拾取插入策略对边缘情况更鲁棒（例如在初始尝试失败后，USB 掉落在主板附近，仍能成功插入），可以干预使策略犯这个错误，然后让它练习从错误中恢复

**训练时间参考**：

在开启随机化并偶尔进行干预的情况下，策略大约需要 **1.5 小时**收敛到 100% 成功率。

**额外提示**：

对于 RAM 插入任务，我们在抓取位姿中添加了额外的随机化。因此，你应该：
- 定期重新抓取 RAM 条
- 按 F1 键
- 将 RAM 放回支架

这有助于提高策略对不同抓取位姿的鲁棒性。

---

## 策略评估

### 步骤 11：评估训练好的策略

1. **编辑评估参数**：

   在 `run_actor.sh` 中添加评估参数：
```bash
--eval_checkpoint_step=CHECKPOINT_NUMBER_TO_EVAL  # 要评估的检查点步数
--eval_n_trajs=N_TIMES_TO_EVAL                    # 评估的轨迹数量
```

2. **运行评估**：
```bash
bash run_actor.sh
```

评估过程中，策略将运行指定次数的轨迹，并统计成功率和其他性能指标。

---

## 训练技巧与注意事项

### 人机干预技巧

1. **使用 SpaceMouse**：
   - 强烈建议使用 SpaceMouse 进行干预，键盘等设备精度不够

2. **干预频率**：
   - 训练初期：频繁干预（每个回合或每隔几个回合）
   - 训练中后期：减少干预频率，只在必要时干预

3. **探索与引导的平衡**：
   - 既要让策略探索（RL 的本质），又要通过干预引导其高效探索
   - 避免过度干预导致策略无法学习

4. **奖励频率**：
   - 训练初期通过干预提高奖励频率，有助于价值函数学习

### 奖励分类器优化

1. **数据质量**：
   - 收集 2-3 倍于正样本的负样本
   - 覆盖所有失败模式

2. **相机配置**：
   - 根据任务难度，可能需要为分类器使用单独的相机
   - 或使用同一相机的多个裁剪视图

3. **阈值调整**：
   - 优先通过收集更多数据来改善分类器
   - 仅在必要时调整阈值

### 配置优化

1. **位姿收集**：
   - 确保所有关键位姿准确收集
   - 探索边界框设置要安全合理

2. **相机配置**：
   - 图像裁剪要覆盖任务相关区域
   - 曝光参数根据环境光照调整

3. **阻抗参数**：
   - `COMPLIANCE_PARAM`：用于一般操作的柔顺参数
   - `PRECISION_PARAM`：用于精确操作的高刚度参数
   - 根据任务需求调整这些参数

### 常见问题排查

1. **分类器误报/漏报**：
   - 收集更多针对性的分类器数据
   - 检查相机视角和图像质量

2. **策略不收敛**：
   - 检查演示数据质量
   - 增加干预频率
   - 检查奖励分类器是否正常工作

3. **机器人行为异常**：
   - 检查阻抗控制器参数
   - 验证末端执行器质量设置
   - 检查探索边界框设置

---

## 总结

RAM 插入任务的训练流程包括：

1. ✅ 机器人设置和服务器启动
2. ✅ 训练配置编辑（URL、相机、位姿）
3. ✅ 奖励分类器数据收集和训练
4. ✅ 人类演示数据收集
5. ✅ 策略训练（Actor + Learner）
6. ✅ 训练过程中的实时干预
7. ✅ 策略评估

通过遵循本指南，你应该能够在约 1.5 小时内训练出一个成功率接近 100% 的 RAM 插入策略。

---

## 相关文件

- 训练配置：`examples/experiments/ram_insertion/config.py`
- 环境包装器：`examples/experiments/ram_insertion/wrapper.py`
- Actor 启动脚本：`examples/experiments/ram_insertion/run_actor.sh`
- Learner 启动脚本：`examples/experiments/ram_insertion/run_learner.sh`
- 主文档：`docs/franka_walkthrough.md`
- 项目 README：`README.md`
